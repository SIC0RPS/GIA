# src/gia/core/utils.py
import os
import sys
import json
import time
import logging
import gc
import hashlib
import queue
import traceback
from typing import List, Dict, Optional, Any
from datetime import datetime
from colorama import Fore, Style
import psutil
import GPUtil
import torch
from tqdm import tqdm
from pathlib import Path
from llama_index.core import (
    VectorStoreIndex, StorageContext, PromptTemplate, SimpleDirectoryReader
)
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.schema import TextNode
from llama_index.core.vector_stores.types import (
    MetadataFilter, MetadataFilters, FilterOperator, FilterCondition
)
from transformers import LogitsProcessor, LogitsProcessorList
from gptqmodel import GPTQModel
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import Settings

from gia.core.logger import logger, log_banner
from gia.config import CONFIG, get_config
from gia.core.state import state_manager
from gia.core.state import load_state
import chromadb

log_banner(__file__)

RULES_PATH = CONFIG["RULES_PATH"]
CONTEXT_WINDOW = CONFIG["CONTEXT_WINDOW"]
MAX_NEW_TOKENS = CONFIG["MAX_NEW_TOKENS"]
TEMPERATURE = CONFIG["TEMPERATURE"]
TOP_P = CONFIG["TOP_P"]
TOP_K = CONFIG["TOP_K"]
REPETITION_PENALTY = CONFIG["REPETITION_PENALTY"]
NO_REPEAT_NGRAM_SIZE = CONFIG["NO_REPEAT_NGRAM_SIZE"]
MODEL_PATH = CONFIG["MODEL_PATH"]
EMBED_MODEL_PATH = CONFIG["EMBED_MODEL_PATH"]
DATA_PATH = CONFIG["DATA_PATH"]
DB_PATH = CONFIG["DB_PATH"]
DEBUG = CONFIG["DEBUG"]
DEVICE_MAP = "cuda" if hasattr(os, 'environ') and os.environ.get('CUDA_VISIBLE_DEVICES') else "cpu"

###

def append_to_chatbot(
    history: List[Dict[str, str]],
    message: str,
    metadata: Optional[Dict] = None,
) -> List[Dict[str, str]]:
    """Append message to chatbot history with role support and validation.

    Args:
        history: Chat history as list of dicts.
        message: Message content to append.
        metadata: Optional dict with 'role' (user/assistant), 'title', etc.

    Returns:
        Updated history.

    Raises:
        ValueError: If inputs are invalid.
    """
    # TO VALIDATE INPUTS
    if not isinstance(history, list):
        raise ValueError("History must be a list of dicts.")
    if not isinstance(message, str) or not message.strip():
        raise ValueError("Message must be a non-empty string.")
    # TO DETECT AND REJECT DUMPS (starts with '[{' and contains 'role' or 'options')
    if message.startswith('[{' ) and ('"role"' in message or '"options"' in message):
        logger.warning(f"Ignoring suspected history dump message: {message[:50]}...")
        return history

    try:
        role = metadata.get("role", "assistant") if metadata else "assistant"
        if role not in ["user", "assistant", "system"]:
            raise ValueError("Role must be 'user', 'assistant', or 'system'.")

        # TO SKIP DUPLICATE OF LAST MESSAGE
        if history and history[-1].get("content") == message and history[-1].get("role") == role:
            return history

        msg_dict: Dict[str, str] = {"role": role, "content": message}
        if metadata:
            # TO EXCLUDE ROLE, OPTIONS, AND OTHER EXTRAS
            msg_dict["metadata"] = {k: v for k, v in metadata.items() if k not in ["role", "options"]}
            if "title" in metadata:
                msg_dict["metadata"]["title"] = "ðŸ› ï¸ Generated by GIA System (GIA.py)"

        history.append(msg_dict)
        return history
    except Exception as e:
        logger.error(f"Error in append_to_chatbot: {e}")
        return history

####


def get_qa_prompt():
    with open(get_config("RULES_PATH"), "r") as f:
        rules = json.load(f)
    qa_prompt_tmpl = "\n".join(rules["qa_prompt"])
    return qa_prompt_tmpl


QA_PROMPT_TMPL = get_qa_prompt()
QA_PROMPT = PromptTemplate(QA_PROMPT_TMPL)


####


def get_system_info():
    # CPU usage
    cpu_usage = psutil.cpu_percent(interval=1)
    # Memory usage
    memory = psutil.virtual_memory()
    memory_usage = memory.percent
    # GPU usage (for NVIDIA GPUs)
    gpus = GPUtil.getGPUs()
    gpu_usage = [gpu.load * 100 for gpu in gpus]
    return cpu_usage, memory_usage, gpu_usage


####

# PRESENCE PENALTY PROCESSOR - FOR CLEANER OUTPUT
class PresencePenaltyLogitsProcessor(LogitsProcessor):
    """Custom logits processor for presence penalty."""

    def __init__(self, presence_penalty: float = 1.0):
        # TO SET PENALTY
        self.presence_penalty = presence_penalty

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        # TO APPLY PENALTY ON GPU
        batch_size, vocab_size = scores.shape
        presence = torch.zeros_like(scores)
        for b in range(batch_size):
            unique_tokens = torch.unique(input_ids[b])
            valid_mask = unique_tokens < vocab_size
            unique_tokens = unique_tokens[valid_mask]
            presence[b, unique_tokens] = self.presence_penalty
        return scores - presence

global_logits_processor = LogitsProcessorList([PresencePenaltyLogitsProcessor()])


def generate(
    query: str,
    system_prompt: str,
    llm: Any,
    *,
    max_new_tokens: Optional[int] = None,
    think: bool = False,
    depth: int = 0,
) -> str:
    """Generate a response using the LLM with OOM handling and performance metrics.

    Args:
        query: The user query string.
        system_prompt: The system prompt to guide the LLM.
        llm: The language model instance.
        max_new_tokens: Optional override for maximum new tokens.
        think: Flag to enable thinking mode.
        depth: Recursion depth for OOM retries.

    Returns:
        Generated response string or error message.
    """
    # STEP 1: CHECK RECURSION DEPTH TO PREVENT STACK OVERFLOW
    if depth > 3:
        return "Error: Max OOM retry depth exceededâ€”clear VRAM manually."
    # STEP 2: PREPARE CHAT MESSAGES WITH CONTROL TAG
    control_tag = "/think" if think else "/no_think"
    chat = [
        ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),
        ChatMessage(role=MessageRole.SYSTEM, content=control_tag),
        ChatMessage(role=MessageRole.USER, content=query),
    ]
    # STEP 3: OVERRIDE MAX TOKENS IF PROVIDED
    previous_max = getattr(llm, "max_new_tokens", None)
    if max_new_tokens is not None:
        llm.max_new_tokens = max_new_tokens
    # STEP 4: START TIMING FOR PERFORMANCE METRICS
    start_time = time.time()  # START TIMING HERE FOR ACCURATE T/S
    try:
        # STEP 5: USE NO_GRAD FOR ZERO-COST GRAD-SKIP
        with torch.no_grad():
            # STEP 6: USE AUTOCAST BFLOAT16 FOR MIXED PRECISION
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                # STEP 7: SET GLOBAL LOGITS PROCESSOR
                llm.generate_kwargs["logits_processor"] = global_logits_processor
                # STEP 8: ENABLE CACHE
                llm.generate_kwargs["use_cache"] = True
                # STEP 9: GENERATE NON-STREAMED RESPONSE
                response = llm.chat(chat, stream=False)
        # STEP 10: EXTRACT RESPONSE CONTENT SAFELY
        streamed_text = ""
        if hasattr(response, "message"):
            streamed_text = response.message.content
        else:
            streamed_text = "Error: empty response"
        # STEP 11: CALCULATE TOKENS AND T/S POST-GENERATION USING TIKTOKEN
        try:
            import tiktoken
            encoding = tiktoken.get_encoding("cl100k_base")
            total_tokens = len(encoding.encode(streamed_text))
        except Exception as e:
            logger.warning(f"Tiktoken failed: {e} - fallback to word split.")
            total_tokens = len(streamed_text.split())  # FALLBACK IF TIKTOKEN ISSUES
        duration = time.time() - start_time
        tokens_per_sec = total_tokens / duration if duration > 0 else 0
        # STEP 12: UPDATE STATE FOR RUNNING AVG TPS (THREAD-SAFE VIA STATE_MANAGER)
        run_count = (state_manager.get_state("run_count") or 0) + 1
        total_tokens_all = (state_manager.get_state("total_tokens_all") or 0) + total_tokens
        total_duration_all = (state_manager.get_state("total_duration_all") or 0.0) + duration
        state_manager.set_state("run_count", run_count)
        state_manager.set_state("total_tokens_all", total_tokens_all)
        state_manager.set_state("total_duration_all", total_duration_all)
        avg_tps = total_tokens_all / total_duration_all if total_duration_all > 0 else 0
        # STEP 13: LOG METRICS WITH INDUSTRY STANDARD TERMS (TOK/S)
        logger.info(
            f"Inference Time: {duration:.2f}s | Tokens: {total_tokens} | Avg TPS: {avg_tps:.2f} tok/s | TPS: {tokens_per_sec:.2f} tok/s"
        )
        return streamed_text
    except torch.cuda.OutOfMemoryError as e:
        # STEP 14: HANDLE OOM WITH VRAM CLEAR AND RETRY
        error_msg = (
            f"Error: OOM during generation - {e}. Reduce max_tokens or clear VRAM."
        )
        print(error_msg, flush=True)
        clear_vram()
        # STEP 15: HALVE MAX TOKENS FOR RETRY
        llm.max_new_tokens = (max_new_tokens or MAX_NEW_TOKENS) // 2
        return generate(
            query,
            system_prompt,
            llm,
            max_new_tokens=llm.max_new_tokens,
            think=think,
            depth=depth + 1,
        )
    except Exception as e:
        # STEP 16: LOG GENERAL ERRORS AND RETURN MESSAGE
        error_msg = f"Error: Unable to generate response: {e}"
        print(error_msg, flush=True)
        return error_msg
    finally:
        # STEP 17: RESTORE ORIGINAL MAX TOKENS AND CLEAR VRAM
        clear_vram()
        if max_new_tokens is not None and previous_max is not None:
            llm.max_new_tokens = previous_max

############################################################################################################


def _init_embed_model(device: str) -> HuggingFaceEmbedding:
    """Initialize embedding model with shared config.

    Args:
        device (str): Device to use ('cuda' or 'cpu').

    Returns:
        HuggingFaceEmbedding: Configured embedding model.
    """
    # INITIALIZE EMBEDDING MODEL WITH SHARED CONFIG
    embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL_PATH,
        max_length=512,
        normalize=True,
        embed_batch_size=256,
        device=device,
    )
    if device == "cuda":
        embed_model._model = embed_model._model.half()  # FP16 FOR VRAM EFFICIENCY
    return embed_model


####


def save_database():
    """Build and persist ChromaDB database with optimized embeddings."""
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    if state.DATABASE_LOADED:
        logger.info("(UDB) Database already loaded, skipping save")
        return
    start_time = time.time()

    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")
        torch.cuda.empty_cache()

        # Initialize ChromaDB client
        db = chromadb.PersistentClient(path=DB_PATH)
        chroma_collection = db.get_or_create_collection(
            "database2", metadata={"hnsw:space": "cosine"}
        )
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        # Configure embedding model
        embed_model = _init_embed_model(device)

        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_documents(
            documents=[],
            storage_context=storage_context,
            embed_model=embed_model,
            show_progress=False,
        )

        # Load and process files
        file_paths = [
            os.path.join(DATA_PATH, f)
            for f in os.listdir(DATA_PATH)
            if f.endswith(".txt")
        ]
        total_files = len(file_paths)
        logger.info(f"Found {total_files} text files to process")

        file_batch_size = 400
        chunk_size = 512
        chunk_overlap = 50
        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for batch_start in tqdm(
            range(0, total_files, file_batch_size), desc="Processing file batches"
        ):
            batch_end = min(batch_start + file_batch_size, total_files)
            batch_paths = file_paths[batch_start:batch_end]
            try:
                logger.info(
                    f"Loading batch {batch_start // file_batch_size + 1} ({batch_start}-{batch_end})"
                )
                documents = SimpleDirectoryReader(
                    input_files=batch_paths, encoding="utf-8"
                ).load_data()
                logger.info(f"Loaded {len(documents)} documents in batch")

                batch_nodes = []
                for doc in documents:
                    chunks = splitter.split_text(doc.text)
                    for chunk in chunks:
                        node_metadata = {
                            "filename": os.path.basename(
                                doc.metadata.get("file_path", "")
                            ),
                            "date": time.strftime("%Y-%m-%d"),
                        }
                        batch_nodes.append(TextNode(text=chunk, metadata=node_metadata))

                logger.info(f"Created {len(batch_nodes)} nodes in batch")
                sub_batch_size = 100
                for sub_start in range(0, len(batch_nodes), sub_batch_size):
                    sub_batch = batch_nodes[
                        sub_start : sub_start + sub_batch_size
                    ]  # FIXED SLICE NOTATION
                    index.insert_nodes(sub_batch)
                    torch.cuda.empty_cache()

                logger.info(f"Completed batch: {len(batch_nodes)} nodes inserted")
                logger.info(
                    f"Current node count in ChromaDB: {chroma_collection.count()}"
                )
                del documents, batch_nodes
            except Exception as e:
                logger.error(
                    f"Error in batch {batch_start // file_batch_size + 1}: {e}"
                )
                torch.cuda.empty_cache()
                continue

        elapsed_time = time.time() - start_time
        logger.info(
            f"ChromaDB database with ~{total_files} files (~{chroma_collection.count()} nodes) created in: {elapsed_time / 3600:.2f} hours"
        )
        # Set state after initialization
        state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
        state_manager.set_state("EMBED_MODEL", embed_model)
        state_manager.set_state("INDEX", index)
        state_manager.set_state("DATABASE_LOADED", True)
    except Exception as e:
        logger.critical(f"Database creation failed: {e}")
        raise


####


def load_database(llm: Optional[object] = None) -> str:
    """One-time ChromaDB initializer."""
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    if state.DATABASE_LOADED:
        logger.info("(UDB) Database already loaded, skipping initialization")
        logger.debug(f"(UDB) Index state: {state.INDEX}")
        return "Database already loaded.\n"

    try:
        logger.info("(UDB) Initializing database")
        clear_vram()
        start_time = time.time()

        # Initialize ChromaDB client
        db = chromadb.PersistentClient(path=DB_PATH)
        chroma_collection = db.get_or_create_collection(
            "database2", metadata={"hnsw:space": "cosine"}
        )

        # Configure bge-large-en-v1.5
        device = "cuda" if torch.cuda.is_available() else "cpu"
        embed_model = _init_embed_model(device)

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)

        # Set index
        index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=embed_model,
            storage_context=storage_context,
            show_progress=True,
        )
        logger.debug(f"(UDB) Index set: {index}")

        if llm is None:
            raise ValueError(
                "Error: no model loaded. Confirm a model before loading the database."
            )

        # Create default query engine with accumulate mode
        query_engine = index.as_query_engine(
            llm=llm,
            similarity_top_k=2,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
            vector_store_query_mode="hybrid",
            response_mode="compact",
            text_qa_template=QA_PROMPT,
            verbose=False,
        )
        logger.debug(f"(UDB) Query engine set: {query_engine}")

        logger.info(
            f"ChromaDB loaded with {chroma_collection.count()} entries in {time.time() - start_time:.2f}s"
        )
        clear_vram()
        # Set state after initialization
        state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
        state_manager.set_state("EMBED_MODEL", embed_model)
        state_manager.set_state("INDEX", index)
        state_manager.set_state("QUERY_ENGINE", query_engine)
        state_manager.set_state("DATABASE_LOADED", True)
        return "Database loaded. Query engine is ready.\n"
    except Exception as e:
        logger.critical(f"Database loading failed: {e}")
        state_manager.set_state("INDEX", None)
        raise


####


def update_database(
    filepaths: list[str] | str,
    query: str,
    metadata: Optional[dict] = None,
    embed_model: Optional[object] = None,
    llm: Optional[object] = None,
) -> None:
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    filepaths = [filepaths] if isinstance(filepaths, str) else filepaths
    if not all(Path(fp).exists() for fp in filepaths):
        raise FileNotFoundError(f"One or more files not found: {filepaths}")

    if state.CHROMA_COLLECTION is None:
        db = chromadb.PersistentClient(path="./db")
        chroma_collection = db.get_or_create_collection(
            "database2",
            metadata={"hnsw:space": "cosine"},
        )
    else:
        chroma_collection = state.CHROMA_COLLECTION

    model = embed_model or state.EMBED_MODEL
    if model is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = _init_embed_model(device)

    splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)

    for filepath in filepaths:
        text = Path(filepath).read_text(encoding="utf-8")
        chunks = splitter.split_text(text)
        if not chunks:
            logger.warning(f"No chunks created for {filepath}")
            continue

        try:
            embeddings = model.get_text_embedding_batch(chunks, show_progress=False)
        except Exception as exc:
            logger.error(f"Embedding failed for {filepath}: {exc}")
            continue

        base_meta = (metadata or {}).copy()
        if "tags" in base_meta:
            if isinstance(base_meta["tags"], str):
                base_meta["tags"] = [
                    t.strip().lower() for t in base_meta["tags"].split(",") if t.strip()
                ]
            elif isinstance(base_meta["tags"], list):
                base_meta["tags"] = [
                    t.lower().strip()
                    for t in base_meta["tags"]
                    if isinstance(t, str) and t.strip()
                ]
            else:
                base_meta["tags"] = []  # FORCE LIST ON INVALID TYPE
        base_meta["filename"] = os.path.basename(filepath)
        if isinstance(base_meta.get("sources"), list):
            base_meta["sources"] = ", ".join(str(s) for s in base_meta["sources"])

        chunk_metadatas = [base_meta.copy() for _ in chunks]
        doc_id = hashlib.md5(filepath.encode("utf-8")).hexdigest()
        chunk_ids = [f"{doc_id}_{i}" for i in range(len(chunks))]

        try:
            chroma_collection.upsert(
                ids=chunk_ids,
                documents=chunks,
                embeddings=embeddings,
                metadatas=chunk_metadatas,
            )
            logger.info(
                f"Upserted {len(chunks)} chunks for {filepath}. "
                f"Collection count: {chroma_collection.count()}"
            )
        except Exception as exc:
            logger.error(f"Upsert failed for {filepath}: {exc}")
    # Set state after initialization
    state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
    state_manager.set_state("DATABASE_LOADED", True)
    state_manager.set_state("EMBED_MODEL", model)


####

def filtered_query_engine(
    llm,
    query_str: str,
    category: str,
) -> RetrieverQueryEngine:
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    """
    Category-restricted query engine with no post-retrieval chunking.
    """


    if state_manager.get_state("INDEX") is None:
        logger.critical("Index not initialized")
        raise ValueError("Index must be initialized")
    if not query_str:
        logger.critical("Query string is empty")
        raise ValueError("query_str must be provided")
    if not category:
        logger.critical("Category is empty")
        raise ValueError("category must be provided")

    logger.debug("(UDB) filtered_query_engine | category=%s", category)

    try:

        meta_filters = MetadataFilters(
            filters=[
                MetadataFilter(
                    key="category",
                    value=category.lower(),
                    operator=FilterOperator.EQ,
                )
            ]
        )


        retriever = state_manager.get_state("INDEX").as_retriever(
            filters=meta_filters,
            similarity_top_k=2,
            verbose=logger.isEnabledFor(logging.DEBUG),
        )


        synthesizer = get_response_synthesizer(
            llm=llm,
            response_mode="compact",  # No chunking; independent per-node
            text_qa_template=QA_PROMPT,
        )


        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=synthesizer,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
        )

        logger.info("(UDB) Filtered query engine created")
        return query_engine

    except torch.cuda.OutOfMemoryError as e:
        logger.warning(
            f"OOM in query engine: {e}. Clearing cache and retrying with top_k=1."
        )
        clear_vram()
        retriever = state_manager.get_state("INDEX").as_retriever(
            filters=meta_filters,
            similarity_top_k=1,  # FALLBACK TO SMALLER TOP_K ON OOM
            verbose=logger.isEnabledFor(logging.DEBUG),
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=synthesizer,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
        )
        return query_engine
    except Exception as exc:
        logger.exception("(UDB) filtered_query_engine FAILED: %s", exc)
        raise RuntimeError(f"Query engine creation failed: {exc}") from exc

###

def clear_vram() -> None:
    """Clear VRAM with retry loop for CUDA errors."""
    original_benchmark = torch.backends.cudnn.benchmark
    torch.backends.cudnn.benchmark = False
    retries = 3
    for attempt in range(retries):
        try:
            if torch.cuda.is_available():
                # TO ENSURE KERNEL COMPLETION BEFORE CLEAR
                torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
            cpu_usage, memory_usage, gpu_usage = get_system_info()  # FIXED UNPACKING
            if memory_usage > 80 or any(g > 80 for g in gpu_usage):
                logger.warning(
                    f"High usage: CPU {cpu_usage}%, RAM {memory_usage}%, GPU {gpu_usage}%â€”cleared, monitor for leaks."
                )
            print(
                f"{Fore.YELLOW}VRAM cleanup executedâ€”CPU: {cpu_usage}%, RAM: {memory_usage}%, GPU: {gpu_usage}%{Style.RESET_ALL}"
            )
            return  # BREAK ON SUCCESS
        except RuntimeError as e:
            if "unknown error" in str(e) and attempt < retries - 1:
                logger.warning(
                    f"CUDA unknown error on attempt {attempt+1}: {e}. Retrying..."
                )
                time.sleep(1)  # SHORT PAUSE TO LET KERNELS SETTLE
                continue
            else:
                logger.error(f"VRAM clear failed after {retries} attempts: {e}")
                raise
        finally:
            # TO RESTORE ORIGINAL BENCHMARK SETTING
            torch.backends.cudnn.benchmark = original_benchmark

###


def clear_ram(
    threshold=80,
):  # CLEAR SYSTEM RAM IF OVER THRESHOLD; NO SPEED HIT IF UNDER
    gc.collect()  # FORCE PYTHON GC; RECLAIMS DEL'D VARS INSTANT
    mem = psutil.virtual_memory().percent
    if mem > threshold and os.geteuid() == 0:  # ROOT-ONLY FOR DROP_CACHES; SKIP IF NOT
        try:
            os.system(
                "sync; echo 3 > /proc/sys/vm/drop_caches"
            )  # FLUSH OS CACHE; FREES RAM WITHOUT THRASH
            logger.info(f"System RAM cleared (was {mem}% > {threshold}%)")
        except Exception as e:
            logger.warning(f"RAM clear failed: {e}. Need sudo?")
    else:
        logger.debug(f"RAM at {mem}% < {threshold}%; skipped clear")

__all__ = [
    "generate",
    "update_database",
    "clear_vram",
    "save_database",
    "load_database",
    "get_system_info",
    "append_to_chatbot",
    "PresencePenaltyLogitsProcessor", 
    "filtered_query_engine",
    "clear_ram",
    # ADD OTHERS IF NEEDED
]