# src/gia/core/utils.py
import os
import sys
import json
import time
import gc
import hashlib
import requests
import importlib
from typing import Tuple, Optional, Dict, Any, Type, List
from datetime import datetime
from colorama import Fore, Style
import psutil
import GPUtil
import torch
from tqdm import tqdm
from pathlib import Path
from llama_index.core import (
    VectorStoreIndex, StorageContext, PromptTemplate, SimpleDirectoryReader
)
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.schema import TextNode
from llama_index.core.vector_stores.types import (
    MetadataFilter, MetadataFilters, FilterOperator, FilterCondition
)
from transformers import LogitsProcessor, LogitsProcessorList
from gptqmodel import GPTQModel
from llama_index.llms.huggingface import HuggingFaceLLM

from llama_index.core import Settings

from huggingface_hub import HfApi
from gia.core.logger import logger, log_banner
from gia.config import CONFIG, get_config
from gia.core.state import state_manager
from gia.core.state import load_state
import chromadb

log_banner(__file__)

RULES_PATH = CONFIG["RULES_PATH"]
CONTEXT_WINDOW = CONFIG["CONTEXT_WINDOW"]
MAX_NEW_TOKENS = CONFIG["MAX_NEW_TOKENS"]
TEMPERATURE = CONFIG["TEMPERATURE"]
TOP_P = CONFIG["TOP_P"]
TOP_K = CONFIG["TOP_K"]
REPETITION_PENALTY = CONFIG["REPETITION_PENALTY"]
NO_REPEAT_NGRAM_SIZE = CONFIG["NO_REPEAT_NGRAM_SIZE"]
MODEL_PATH = CONFIG["MODEL_PATH"]
EMBED_MODEL_PATH = CONFIG["EMBED_MODEL_PATH"]
DATA_PATH = CONFIG["DATA_PATH"]
DB_PATH = CONFIG["DB_PATH"]
DEBUG = CONFIG["DEBUG"]
DEVICE_MAP = "cuda" if hasattr(os, 'environ') and os.environ.get('CUDA_VISIBLE_DEVICES') else "cpu"


from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

###

def append_to_chatbot(
    history: List[Dict[str, str]],
    message: str,
    metadata: Optional[Dict] = None,
) -> List[Dict[str, str]]:
    """Append message to chatbot history with role support and validation.

    Args:
        history: Chat history as list of dicts.
        message: Message content to append.
        metadata: Optional dict with 'role' (user/assistant), 'title', etc.

    Returns:
        Updated history.

    Raises:
        ValueError: If inputs are invalid.
    """
    # TO VALIDATE INPUTS
    if not isinstance(history, list):
        raise ValueError("History must be a list of dicts.")
    if not isinstance(message, str) or not message.strip():
        raise ValueError("Message must be a non-empty string.")
    # TO DETECT AND REJECT DUMPS (starts with '[{' and contains 'role' or 'options')
    if message.startswith('[{' ) and ('"role"' in message or '"options"' in message):
        logger.warning(f"Ignoring suspected history dump message: {message[:50]}...")
        return history

    try:
        role = metadata.get("role", "assistant") if metadata else "assistant"
        if role not in ["user", "assistant", "system"]:
            raise ValueError("Role must be 'user', 'assistant', or 'system'.")

        # TO SKIP DUPLICATE OF LAST MESSAGE
        if history and history[-1].get("content") == message and history[-1].get("role") == role:
            return history

        msg_dict: Dict[str, str] = {"role": role, "content": message}
        if metadata:
            # TO EXCLUDE ROLE, OPTIONS, AND OTHER EXTRAS
            msg_dict["metadata"] = {k: v for k, v in metadata.items() if k not in ["role", "options"]}
            if "title" in metadata:
                msg_dict["metadata"]["title"] = "ðŸ› ï¸ Generated by GIA System (GIA.py)"

        history.append(msg_dict)
        return history
    except Exception as e:
        logger.error(f"Error in append_to_chatbot: {e}")
        return history

####


def get_qa_prompt():
    with open(get_config("RULES_PATH"), "r") as f:
        rules = json.load(f)
    qa_prompt_tmpl = "\n".join(rules["qa_prompt"])
    return qa_prompt_tmpl


QA_PROMPT_TMPL = get_qa_prompt()
QA_PROMPT = PromptTemplate(QA_PROMPT_TMPL)


####


def get_system_info():
    # CPU usage
    cpu_usage = psutil.cpu_percent(interval=1)
    # Memory usage
    memory = psutil.virtual_memory()
    memory_usage = memory.percent
    # GPU usage (for NVIDIA GPUs)
    gpus = GPUtil.getGPUs()
    gpu_usage = [gpu.load * 100 for gpu in gpus]
    return cpu_usage, memory_usage, gpu_usage


####

# PRESENCE PENALTY PROCESSOR - FOR CLEANER OUTPUT
class PresencePenaltyLogitsProcessor(LogitsProcessor):
    """Custom logits processor for presence penalty."""

    def __init__(self, presence_penalty: float = 1.0):
        # TO SET PENALTY
        self.presence_penalty = presence_penalty

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -> torch.FloatTensor:
        # TO APPLY PENALTY ON GPU
        batch_size, vocab_size = scores.shape
        presence = torch.zeros_like(scores)
        for b in range(batch_size):
            unique_tokens = torch.unique(input_ids[b])
            valid_mask = unique_tokens < vocab_size
            unique_tokens = unique_tokens[valid_mask]
            presence[b, unique_tokens] = self.presence_penalty
        return scores - presence

global_logits_processor = LogitsProcessorList([PresencePenaltyLogitsProcessor()])

from llama_index.core.llms import LLM, ChatMessage, MessageRole
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.llms.huggingface import HuggingFaceLLM

# In utils.py
from typing import Optional
from llama_index.core.llms import LLM, ChatMessage, MessageRole
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.llms.huggingface import HuggingFaceLLM




####

def _init_embed_model(device: str) -> HuggingFaceEmbedding:
    """Initialize embedding model with shared config.

    Args:
        device (str): Device to use ('cuda' or 'cpu').

    Returns:
        HuggingFaceEmbedding: Configured embedding model.
    """
    # INITIALIZE EMBEDDING MODEL WITH SHARED CONFIG
    embed_model = HuggingFaceEmbedding(
        model_name=EMBED_MODEL_PATH,
        max_length=512,
        normalize=True,
        embed_batch_size=256,
        device=device,
    )
    if device == "cuda":
        embed_model._model = embed_model._model.half()  # FP16 FOR VRAM EFFICIENCY
    return embed_model

####

def save_database():
    """Build and persist ChromaDB database with optimized embeddings."""
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    if state.DATABASE_LOADED:
        logger.info("(UDB) Database already loaded, skipping save")
        return
    start_time = time.time()

    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")
        torch.cuda.empty_cache()

        # Initialize ChromaDB client
        db = chromadb.PersistentClient(path=DB_PATH)
        chroma_collection = db.get_or_create_collection(
            "database2", metadata={"hnsw:space": "cosine"}
        )
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        # Configure embedding model
        embed_model = _init_embed_model(device)

        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_documents(
            documents=[],
            storage_context=storage_context,
            embed_model=embed_model,
            show_progress=False,
        )

        # Load and process files
        file_paths = [
            os.path.join(DATA_PATH, f)
            for f in os.listdir(DATA_PATH)
            if f.endswith(".txt")
        ]
        total_files = len(file_paths)
        logger.info(f"Found {total_files} text files to process")

        file_batch_size = 400
        chunk_size = 512
        chunk_overlap = 50
        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for batch_start in tqdm(
            range(0, total_files, file_batch_size), desc="Processing file batches"
        ):
            batch_end = min(batch_start + file_batch_size, total_files)
            batch_paths = file_paths[batch_start:batch_end]
            try:
                logger.info(
                    f"Loading batch {batch_start // file_batch_size + 1} ({batch_start}-{batch_end})"
                )
                documents = SimpleDirectoryReader(
                    input_files=batch_paths, encoding="utf-8"
                ).load_data()
                logger.info(f"Loaded {len(documents)} documents in batch")

                batch_nodes = []
                for doc in documents:
                    chunks = splitter.split_text(doc.text)
                    for chunk in chunks:
                        node_metadata = {
                            "filename": os.path.basename(
                                doc.metadata.get("file_path", "")
                            ),
                            "date": time.strftime("%Y-%m-%d"),
                        }
                        batch_nodes.append(TextNode(text=chunk, metadata=node_metadata))

                logger.info(f"Created {len(batch_nodes)} nodes in batch")
                sub_batch_size = 100
                for sub_start in range(0, len(batch_nodes), sub_batch_size):
                    sub_batch = batch_nodes[
                        sub_start : sub_start + sub_batch_size
                    ]  # FIXED SLICE NOTATION
                    index.insert_nodes(sub_batch)
                    torch.cuda.empty_cache()

                logger.info(f"Completed batch: {len(batch_nodes)} nodes inserted")
                logger.info(
                    f"Current node count in ChromaDB: {chroma_collection.count()}"
                )
                del documents, batch_nodes
            except Exception as e:
                logger.error(
                    f"Error in batch {batch_start // file_batch_size + 1}: {e}"
                )
                torch.cuda.empty_cache()
                continue

        elapsed_time = time.time() - start_time
        logger.info(
            f"ChromaDB database with ~{total_files} files (~{chroma_collection.count()} nodes) created in: {elapsed_time / 3600:.2f} hours"
        )
        # Set state after initialization
        state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
        state_manager.set_state("EMBED_MODEL", embed_model)
        state_manager.set_state("INDEX", index)
        state_manager.set_state("DATABASE_LOADED", True)
    except Exception as e:
        logger.critical(f"Database creation failed: {e}")
        raise

####

def load_database(llm: Optional[object] = None) -> str:
    """One-time ChromaDB initializer."""
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    if state.DATABASE_LOADED:
        logger.info("(UDB) Database already loaded, skipping initialization")
        logger.debug(f"(UDB) Index state: {state.INDEX}")
        return "Database already loaded.\n"

    try:
        logger.info("(UDB) Initializing database")
        clear_vram()
        start_time = time.time()

        # Initialize ChromaDB client
        db = chromadb.PersistentClient(path=DB_PATH)
        chroma_collection = db.get_or_create_collection(
            "database2", metadata={"hnsw:space": "cosine"}
        )

        # Configure bge-large-en-v1.5
        device = "cuda" if torch.cuda.is_available() else "cpu"
        embed_model = _init_embed_model(device)

        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)

        # Set index
        index = VectorStoreIndex.from_vector_store(
            vector_store,
            embed_model=embed_model,
            storage_context=storage_context,
            show_progress=True,
        )
        logger.debug(f"(UDB) Index set: {index}")

        if llm is None:
            raise ValueError(
                "Error: no model loaded. Confirm a model before loading the database."
            )

        # Create default query engine with accumulate mode
        query_engine = index.as_query_engine(
            llm=llm,
            similarity_top_k=2,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
            vector_store_query_mode="hybrid",
            response_mode="compact",
            text_qa_template=QA_PROMPT,
            verbose=False,
        )
        logger.debug(f"(UDB) Query engine set: {query_engine}")

        logger.info(
            f"ChromaDB loaded with {chroma_collection.count()} entries in {time.time() - start_time:.2f}s"
        )
        clear_vram()
        # Set state after initialization
        state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
        state_manager.set_state("EMBED_MODEL", embed_model)
        state_manager.set_state("INDEX", index)
        state_manager.set_state("QUERY_ENGINE", query_engine)
        state_manager.set_state("DATABASE_LOADED", True)
        return "Database loaded. Query engine is ready.\n"
    except Exception as e:
        logger.critical(f"Database loading failed: {e}")
        state_manager.set_state("INDEX", None)
        raise

####

def update_database(
    filepaths: list[str] | str,
    query: str,
    metadata: Optional[dict] = None,
    embed_model: Optional[object] = None,
    llm: Optional[object] = None,
) -> None:
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    filepaths = [filepaths] if isinstance(filepaths, str) else filepaths
    if not all(Path(fp).exists() for fp in filepaths):
        raise FileNotFoundError(f"One or more files not found: {filepaths}")

    if state.CHROMA_COLLECTION is None:
        db = chromadb.PersistentClient(path="./db")
        chroma_collection = db.get_or_create_collection(
            "database2",
            metadata={"hnsw:space": "cosine"},
        )
    else:
        chroma_collection = state.CHROMA_COLLECTION

    model = embed_model or state.EMBED_MODEL
    if model is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = _init_embed_model(device)

    splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)

    for filepath in filepaths:
        text = Path(filepath).read_text(encoding="utf-8")
        chunks = splitter.split_text(text)
        if not chunks:
            logger.warning(f"No chunks created for {filepath}")
            continue

        try:
            embeddings = model.get_text_embedding_batch(chunks, show_progress=False)
        except Exception as exc:
            logger.error(f"Embedding failed for {filepath}: {exc}")
            continue

        base_meta = (metadata or {}).copy()
        if "tags" in base_meta:
            if isinstance(base_meta["tags"], str):
                base_meta["tags"] = [
                    t.strip().lower() for t in base_meta["tags"].split(",") if t.strip()
                ]
            elif isinstance(base_meta["tags"], list):
                base_meta["tags"] = [
                    t.lower().strip()
                    for t in base_meta["tags"]
                    if isinstance(t, str) and t.strip()
                ]
            else:
                base_meta["tags"] = []  # FORCE LIST ON INVALID TYPE
        base_meta["filename"] = os.path.basename(filepath)
        if isinstance(base_meta.get("sources"), list):
            base_meta["sources"] = ", ".join(str(s) for s in base_meta["sources"])

        chunk_metadatas = [base_meta.copy() for _ in chunks]
        doc_id = hashlib.md5(filepath.encode("utf-8")).hexdigest()
        chunk_ids = [f"{doc_id}_{i}" for i in range(len(chunks))]

        try:
            chroma_collection.upsert(
                ids=chunk_ids,
                documents=chunks,
                embeddings=embeddings,
                metadatas=chunk_metadatas,
            )
            logger.info(
                f"Upserted {len(chunks)} chunks for {filepath}. "
                f"Collection count: {chroma_collection.count()}"
            )
        except Exception as exc:
            logger.error(f"Upsert failed for {filepath}: {exc}")
    # Set state after initialization
    state_manager.set_state("CHROMA_COLLECTION", chroma_collection)
    state_manager.set_state("DATABASE_LOADED", True)
    state_manager.set_state("EMBED_MODEL", model)


def unload_model() -> str:
    """Unload the current model and clear resources.

    Returns:
        str: Status message.
    """
    # TO RETRIEVE CURRENT LLM FROM STATE
    llm = state_manager.get_state("LLM")
    if llm is None:
        return "No model loaded to unload."
    try:
        # TO HANDLE HUGGINGFACE-SPECIFIC UNLOAD
        if isinstance(llm, HuggingFaceLLM):
            if hasattr(llm, "_model"):
                del llm._model
            if hasattr(llm, "_tokenizer"):
                del llm._tokenizer
            del llm
            clear_vram()
        else:
            # TO HANDLE ONLINE/OTHER LLMS
            del llm
            gc.collect()
        # TO RESET STATE KEYS
        state_manager.set_state("LLM", None)
        state_manager.set_state("MODEL_NAME", None)
        state_manager.set_state("MODEL_PATH", None)
        return "Model unloaded successfully."
    except Exception as e:
        logger.error(f"Unload failed: {e}")
        return f"Error unloading model: {str(e)}"

####

def filtered_query_engine(
    llm,
    query_str: str,
    category: str,
) -> RetrieverQueryEngine:
    state = load_state()  # LOAD STATE AT START FOR DOT ACCESS
    """
    Category-restricted query engine with no post-retrieval chunking.
    """


    if state_manager.get_state("INDEX") is None:
        logger.critical("Index not initialized")
        raise ValueError("Index must be initialized")
    if not query_str:
        logger.critical("Query string is empty")
        raise ValueError("query_str must be provided")
    if not category:
        logger.critical("Category is empty")
        raise ValueError("category must be provided")

    logger.debug("(UDB) filtered_query_engine | category=%s", category)

    try:

        meta_filters = MetadataFilters(
            filters=[
                MetadataFilter(
                    key="category",
                    value=category.lower(),
                    operator=FilterOperator.EQ,
                )
            ]
        )


        retriever = state_manager.get_state("INDEX").as_retriever(
            filters=meta_filters,
            similarity_top_k=2,
            verbose=logger.isEnabledFor(logger.DEBUG),
        )


        synthesizer = get_response_synthesizer(
            llm=llm,
            response_mode="compact",  # No chunking; independent per-node
            text_qa_template=QA_PROMPT,
        )


        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=synthesizer,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
        )

        logger.info("(UDB) Filtered query engine created")
        return query_engine

    except torch.cuda.OutOfMemoryError as e:
        logger.warning(
            f"OOM in query engine: {e}. Clearing cache and retrying with top_k=1."
        )
        clear_vram()
        retriever = state_manager.get_state("INDEX").as_retriever(
            filters=meta_filters,
            similarity_top_k=1,  # FALLBACK TO SMALLER TOP_K ON OOM
            verbose=logger.isEnabledFor(logger.DEBUG),
        )
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=synthesizer,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.65)],
        )
        return query_engine
    except Exception as exc:
        logger.exception("(UDB) filtered_query_engine FAILED: %s", exc)
        raise RuntimeError(f"Query engine creation failed: {exc}") from exc

###

def clear_vram() -> None:
    """Clear VRAM with retry loop for CUDA errors."""
    original_benchmark = torch.backends.cudnn.benchmark
    torch.backends.cudnn.benchmark = False
    retries = 3
    for attempt in range(retries):
        try:
            if torch.cuda.is_available():
                # TO ENSURE KERNEL COMPLETION BEFORE CLEAR
                torch.cuda.synchronize()
            torch.cuda.empty_cache()
            gc.collect()
            cpu_usage, memory_usage, gpu_usage = get_system_info()  # FIXED UNPACKING
            if memory_usage > 80 or any(g > 80 for g in gpu_usage):
                logger.warning(
                    f"High usage: CPU {cpu_usage}%, RAM {memory_usage}%, GPU {gpu_usage}%â€”cleared, monitor for leaks."
                )
            logger.info(
                f"VRAM clearedâ€”CPU: {cpu_usage}%, RAM: {memory_usage}%, GPU: {gpu_usage}%"
            )
            return  # BREAK ON SUCCESS
        except RuntimeError as e:
            if "unknown error" in str(e) and attempt < retries - 1:
                logger.warning(
                    f"CUDA unknown error on attempt {attempt+1}: {e}. Retrying..."
                )
                time.sleep(1)  # SHORT PAUSE TO LET KERNELS SETTLE
                continue
            else:
                logger.error(f"VRAM clear failed after {retries} attempts: {e}")
                raise
        finally:
            # TO RESTORE ORIGINAL BENCHMARK SETTING
            torch.backends.cudnn.benchmark = original_benchmark

def clear_ram(
    threshold=80,
):  # CLEAR SYSTEM RAM IF OVER THRESHOLD; NO SPEED HIT IF UNDER
    gc.collect()  # FORCE PYTHON GC; RECLAIMS DEL'D VARS INSTANT
    mem = psutil.virtual_memory().percent
    if mem > threshold and os.geteuid() == 0:  # ROOT-ONLY FOR DROP_CACHES; SKIP IF NOT
        try:
            os.system(
                "sync; echo 3 > /proc/sys/vm/drop_caches"
            )  # FLUSH OS CACHE; FREES RAM WITHOUT THRASH
            logger.info(f"System RAM cleared (was {mem}% > {threshold}%)")
        except Exception as e:
            logger.warning(f"RAM clear failed: {e}. Need sudo?")
    else:
        logger.debug(f"RAM at {mem}% < {threshold}%; skipped clear")

def fetch_openai_models() -> list:
    """Fetch available OpenAI models from the API for dropdown choices, only valid chat/completion models."""
    valid_models = [
        "o1", "o1-2024-12-17", "o1-pro", "o1-pro-2025-03-19", "o1-preview", "o1-preview-2024-09-12", "o1-mini", "o1-mini-2024-09-12",
        "o3-mini", "o3-mini-2025-01-31", "o3", "o3-2025-04-16", "o3-pro", "o3-pro-2025-06-10", "o4-mini", "o4-mini-2025-04-16",
        "gpt-4", "gpt-4-32k", "gpt-4-1106-preview", "gpt-4-0125-preview", "gpt-4-turbo-preview", "gpt-4-vision-preview", "gpt-4-1106-vision-preview",
        "gpt-4-turbo-2024-04-09", "gpt-4-turbo", "gpt-4o", "gpt-4o-audio-preview", "gpt-4o-audio-preview-2024-12-17", "gpt-4o-audio-preview-2024-10-01",
        "gpt-4o-mini-audio-preview", "gpt-4o-mini-audio-preview-2024-12-17", "gpt-4o-2024-05-13", "gpt-4o-2024-08-06", "gpt-4o-2024-11-20",
        "gpt-4.5-preview", "gpt-4.5-preview-2025-02-27", "chatgpt-4o-latest", "gpt-4o-mini", "gpt-4o-mini-2024-07-18", "gpt-4-0613",
        "gpt-4-32k-0613", "gpt-4-0314", "gpt-4-32k-0314", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano", "gpt-4.1-2025-04-14",
        "gpt-4.1-mini-2025-04-14", "gpt-4.1-nano-2025-04-14", "gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-3.5-turbo-0125", "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-0613", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-0301", "text-davinci-003", "text-davinci-002", "gpt-3.5-turbo-instruct",
        "text-ada-001", "text-babbage-001", "text-curie-001", "ada", "babbage", "curie", "davinci", "gpt-35-turbo-16k", "gpt-35-turbo",
        "gpt-35-turbo-0125", "gpt-35-turbo-1106", "gpt-35-turbo-0613", "gpt-35-turbo-16k-0613"
    ]
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        return valid_models
    try:
        headers = {"Authorization": f"Bearer {api_key}"}
        resp = requests.get("https://api.openai.com/v1/models", headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        # Only include valid models present in the API response
        available = set(m["id"] for m in data.get("data", []))
        filtered = [m for m in valid_models if m in available]
        return filtered or valid_models
    except Exception as e:
        logger.warning(f"Could not fetch OpenAI models for dropdown: {e}")
        return valid_models


def create_llm(engine: str, model_name: str) -> LLM:
    """Dynamically create LLM instance based on engine with shared config.

    Args:
        engine: LLM engine (e.g., 'HuggingFaceLLM', 'LlamaOpenAI').
        model_name: Model name or path.

    Returns:
        LLM instance.
    """
    mode = engine
    logger.debug(f"Creating LLM for engine: {engine}, model: {model_name}")
    if mode == "Local":
        # TO LOAD LOCAL MODEL WITH GPTQMODEL
        model_path = model_name
        pretrained_model = GPTQModel.from_quantized(
            model_path,
            device_map=DEVICE_MAP,
            use_safetensors=True,
            trust_remote_code=True,
            inject_fused_attention=False,
            inject_fused_mlp=False,
        )
        tokenizer = pretrained_model.tokenizer
        # TO SUPPRESS PAD TOKEN WARNING
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        llm = HuggingFaceLLM(
            model=pretrained_model.model,
            tokenizer=tokenizer,
            context_window=CONTEXT_WINDOW,
            max_new_tokens=MAX_NEW_TOKENS,
            system_prompt="",  # SET LATER
            query_wrapper_prompt="",  # SET LATER
            generate_kwargs={
                "temperature": TEMPERATURE,
                "top_p": TOP_P,
                "top_k": TOP_K,
                "repetition_penalty": REPETITION_PENALTY,
                "no_repeat_ngram_size": NO_REPEAT_NGRAM_SIZE,
                "do_sample": True,
                "logits_processor": global_logits_processor,
            },
            device_map=DEVICE_MAP,
        )
        # TO SET GENERATION CONFIG FOR PAD TOKEN
        llm._model.generation_config.pad_token_id = tokenizer.eos_token_id
        use_chat = True
    elif mode == "HuggingFace":
        llm = HuggingFaceInferenceAPI(
            model_name=model_name,
            temperature=TEMPERATURE,
            max_tokens=MAX_NEW_TOKENS,
            top_p=TOP_P,
            api_key=os.getenv("HF_TOKEN"),
            additional_kwargs={"repetition_penalty": REPETITION_PENALTY},
        )
        use_chat = False  # INFERENCE API MODE DISABLED DUE TO VERSION ISSUES WITH OPENAILIKE
    else:
        try:
            module_name = engine.replace("LLM", "").lower()
            class_name = engine
            if module_name == "llamaopenai":
                module_name = "openai"
                class_name = "OpenAI"
            if not module_name.startswith(("openai", "huggingface", "anthropic", "groq", "mistralai", "cohere", "bedrock")):
                raise ValueError(f"Invalid engine module '{module_name}'. Supported patterns: openai*, huggingface*, anthropic, groq, mistralai, cohere, bedrock.")
            module = importlib.import_module(f"llama_index.llms.{module_name}")
            llm_class: Type[LLM] = getattr(module, class_name)
            init_kwargs = {
                "model": model_name,
                "temperature": TEMPERATURE,
                "max_tokens": MAX_NEW_TOKENS,
                "top_p": TOP_P,
                "presence_penalty": REPETITION_PENALTY,
            }
            init_vars = llm_class.__init__.__code__.co_varnames
            if "api_key" in init_vars:
                api_key = os.getenv(f"{class_name.upper()}_API_KEY")
                if api_key:
                    init_kwargs["api_key"] = api_key
            if "token" in init_vars:
                token = os.getenv("HF_TOKEN") or os.getenv(f"{class_name.upper()}_TOKEN")
                if token:
                    init_kwargs["token"] = token
            if "api_base" in init_vars:
                init_kwargs["api_base"] = os.getenv("API_BASE", "")
            llm = llm_class(**init_kwargs)
            use_chat = True
            logger.debug(f"Dynamic engine {engine} loaded for {model_name}")
        except (ImportError, AttributeError, ValueError) as e:
            logger.debug(f"Dynamic load failed for '{engine}': {e}")
            raise ValueError(f"Dynamic load failed for '{engine}': {e}") from e

    state_manager.set_state("USE_CHAT", use_chat)
    logger.debug(f"LLM creation complete; USE_CHAT: {use_chat}")
    return llm

import contextlib  # TO REDIRECT STDOUT SAFELY DURING MODEL LOAD (FOR CAPTURING GPTQMODEL PRINTS)
from gia.core.logger import log_file  # TO ACCESS LOG FILE FOR REDIRECTION

def load_llm(mode: str, config: Dict) -> Tuple[str, Optional[Any]]:
    """Load LLM based on mode with config.
    Args:
        mode: "Local", "HuggingFace", "OpenAI", "OpenRouter".
        config: Dict with "model_path" for local or "model_name" for online.
    Returns:
        Tuple[str, Optional[LLM]]: Message and loaded LLM.
    """
    logger.debug(f"Loading LLM for mode: {mode} with config: {config}")
    if mode not in ["Local", "HuggingFace", "OpenAI", "OpenRouter"]:
        logger.debug(f"Invalid mode: {mode}")
        return "Invalid mode.", None
    model_name = config.get("model_name") or config.get("model_path")
    if not model_name:
        logger.debug("No model name or path in config")
        return "No model provided for mode.", None
    try:
        # TO REDIRECT GPTQMODEL STDOUT LOGS TO SECONDARY TERMINAL (APP_LOGS.TXT) DURING LOCAL LOAD
        if mode == "Local":
            with open(log_file, 'a') as f_log:
                with contextlib.redirect_stdout(f_log):
                    llm = create_llm(mode, model_name)
        else:
            llm = create_llm(mode, model_name)
        state_manager.set_state("MODEL_NAME", model_name if mode != "Local" else os.path.basename(model_name))
        state_manager.set_state("MODEL_PATH", model_name if mode == "Local" else None)
        logger.debug(f"Load success for {mode}: {model_name}")
        return f"{mode} model loaded: {model_name}", llm
    except Exception as e:
        logger.debug(f"Load failed for {mode}: {str(e)}")
        return f"Failed to load {mode} model: {str(e)}", None


def generate(
    query: str,
    system_prompt: str,
    llm: Any,
    *,
    max_new_tokens: Optional[int] = None,
    think: bool = False,
    depth: int = 0,
    **kwargs: Any,
) -> str:
    """Generate a response using the LLM with OOM handling and performance metrics."""
    if depth > 3:
        return "Error: Max OOM retry depth exceededâ€”clear VRAM manually."
    query = query.replace('<', '').replace('>', '').replace('"', '').replace("'", '')
    control_tag = "/think" if think else "/no_think"
    full_prompt = f"{system_prompt}\n{control_tag}\n{query}"
    model_name = kwargs.get("model")
    engine = kwargs.get("engine")
    if model_name and engine:
        try:
            llm = create_llm(engine, model_name)
        except Exception as e:
            logger.warning(f"Dynamic LLM load failed for '{engine}': {e}")
            return f"Error loading specified model: {str(e)}"
    previous_max = getattr(llm, "max_new_tokens", None) if hasattr(llm, "max_new_tokens") else None
    if max_new_tokens is not None and hasattr(llm, "max_new_tokens"):
        llm.max_new_tokens = max_new_tokens
    start_time = time.time()
    try:
        with torch.no_grad():
            kwargs_gen = {}
            if max_new_tokens is not None and isinstance(llm, LlamaOpenAI):
                kwargs_gen["max_tokens"] = max_new_tokens
            use_chat = state_manager.get_state("USE_CHAT", True)
            if use_chat:
                chat_msgs = [
                    ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),
                    ChatMessage(role=MessageRole.SYSTEM, content=control_tag),
                    ChatMessage(role=MessageRole.USER, content=query),
                ]
                response = llm.chat(chat_msgs, **kwargs_gen)
                streamed_text = response.message.content
                if not streamed_text.strip():
                    logger.warning("Empty chat responseâ€”falling back to complete.")
                    response = llm.complete(full_prompt, **kwargs_gen)
                    streamed_text = response.text
            else:
                response = llm.complete(full_prompt, **kwargs_gen)
                streamed_text = response.text
            if not streamed_text.strip():
                logger.warning("Empty response from modelâ€”check query, provider limits, or model compatibility.")
                return "Empty response from model."
        try:
            import tiktoken
            encoding = tiktoken.get_encoding("cl100k_base")
            total_tokens = len(encoding.encode(streamed_text))
        except Exception as e:
            logger.warning(f"Tiktoken failed: {e} - fallback to word split.")
            total_tokens = len(streamed_text.split())
        duration = time.time() - start_time
        tokens_per_sec = total_tokens / duration if duration > 0 else 0
        run_count = (state_manager.get_state("run_count") or 0) + 1
        total_tokens_all = (state_manager.get_state("total_tokens_all") or 0) + total_tokens
        total_duration_all = (state_manager.get_state("total_duration_all") or 0.0) + duration
        state_manager.set_state("run_count", run_count)
        state_manager.set_state("total_tokens_all", total_tokens_all)
        state_manager.set_state("total_duration_all", total_duration_all)
        avg_tps = total_tokens_all / total_duration_all if total_duration_all > 0 else 0
        logger.info(
            f"Inference Time: {duration:.2f}s | Tokens: {total_tokens} | Avg TPS: {avg_tps:.2f} tok/s | TPS: {tokens_per_sec:.2f} tok/s"
        )
        return streamed_text.strip()
    except torch.cuda.OutOfMemoryError as e:
        logger.warning(f"Error: OOM during generation - {e}. Reduce max_tokens or clear VRAM.")
        clear_vram()
        if hasattr(llm, "max_new_tokens"):
            llm.max_new_tokens = (max_new_tokens or MAX_NEW_TOKENS) // 2
        return generate(
            query,
            system_prompt,
            llm,
            max_new_tokens=(llm.max_new_tokens if hasattr(llm, "max_new_tokens") else None),
            think=think,
            depth=depth + 1,
            **kwargs,
        )
    except Exception as e:
        error_msg = f"Error: Unable to generate response: {e}"
        logger.error(error_msg)
        return error_msg
    finally:
        clear_vram()
        if max_new_tokens is not None and previous_max is not None and hasattr(llm, "max_new_tokens"):
            llm.max_new_tokens = previous_max

__all__ = [
    "generate",
    "update_database",
    "clear_vram",
    "save_database",
    "load_database",
    "get_system_info",
    "append_to_chatbot",
    "PresencePenaltyLogitsProcessor", 
    "filtered_query_engine",
    "clear_ram",
    "fetch_openai_models",
    "create_llm",
    "load_llm",
    "unload_model",
]
